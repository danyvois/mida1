%!TEX root = ../main.tex

What we did \eqref{eq:reconstruction-noise-from-data} is indeed possible (meaning we can reconstruct $e(t)$ from $y(t)$) thanks to the further assumptions we made at page \pageref{assumptions-prediction-theory}. In fact, this means that $W(z)^{-1}=\frac{A(z)}{C(z)}$ is asymptotically stable too.

From the point of view of transfer functions we have that:
\[
	\hat y (t+k\mid t) = \frac{F(z)}{A(z)}e(t) =\frac{F(z)}{\cancel{A(z)}}\cdot\frac{\cancel{A(z)}}{C(z)}y(t) =\frac{F(z)}{C(z)}y(t) \implies \boxed{\hat y (t+k\mid t) = \frac{F(z)}{C(z)}y(t)}
\]
meaning that the optimal predictor from output is obtained as the output of a digital filter $F(z)/C(z)$ fed by $y(t)$ up to time $t$.

\textbf{Remark.}
The correct expression of the linear predictor can \emph{only} be obtained by using the canonical representation, otherwise there may be zeros of $C(z)$ which becomes unstable poles when reconstructing $e(t)$.

Let us illustrate this fact through an example.

\begin{exa}
Let $e(t)\sim \WN(0,1)$ and consider MA($1$) process:
\begin{align*}
	y(t) &= e(t) - 2 e(t-1)\\
	&= (1-2z^{-1} )e(t) &\text{non-canonical}\\
	&=\left( 1-\frac{1}{2} z^{-1}  \right) \cdot\frac{1-2z^{-1}}{1-\frac{1}{2} z^{-1}} e(t)\\
	&=\left( 1-\frac{1}{2} z^{-1}  \right) \xi(t) &\text{canonical}
\end{align*}
where $\xi(t)\sim \WN(0,4)$. The first form was non-canonical because there was a zero ($z=2$) outside the unit circle.

The original process (non-canonical) can be written as
\[
	y(t+1)=\underbrace{e(t+1)}_{\substack{\text{unpred.}\\\text{at $t$}}}-\underbrace{2e(t)}_{\substack{\text{pred.}\\\text{at $t$}}}
\]
thus our optimal predictor from noise would be
\[
	\hat y^{e} (t+1\mid t) = -2e(t)
\]
Whereas the process in canonical form can be written as 
\[
	y(t+1) =\underbrace{\xi(t+1)}_{\substack{\text{unpred.}\\\text{at $t$}}}-\underbrace{\frac{1}{2} \xi(t)}_{\substack{\text{pred.}\\\text{at $t$}}}
\]
thus our optimal predictor from noise would be
\[
	\hat y^{\xi} (t+1\mid t)=-\frac{1}{2} \xi(t).
\]
The problem now is that if we try to reconstruct the noise from the output in the predictor $\hat y^{e} (t+1\mid t) = -2e(t)$ obtained by the non canonical form, we get:
\begin{align*}
	y(t)=(1-2z^{-1})e(t) \iff & e(t) =\frac{1}{1-2z^{-1}} y(t)\\
	\implies & \hat y^{e} (t+1\mid t) = -2e(t) = -\frac{2}{1-2z^{-1}}y(t)
\end{align*}
this is a \textbf{fatal mistake}, because it's not well defined, $e(t)$ cannot be reconstructed here!

The correct way is indeed using the canonical representation:
\begin{align*}
	y(t) = \left( 1-\frac{1}{2} z^{-1}  \right) \xi(t) \iff & \xi(t) = \frac{1}{1-\frac{1}{2} z^{-1} } y(t)\\
	\implies & \boxed{\hat y^{\xi} (t+1\mid t)= -\frac{1}{2} \xi(t) = \frac{-\frac{1}{2} }{1-\frac{1}{2} z^{-1} } y(t)}
\end{align*}
If we consider the prediction error we see that they are different:
\[
	\begin{rcases}
		\E\left[\left( y(t+1)-\hat    y^{e}(t+1\mid t) \right)^2 \right] = \E[e(t+1)^2]   = 1\\
		\E\left[\left( y(t+1)-\hat y ^{\xi}(t+1\mid t) \right)^2 \right] = \E[\xi(t+1)^2] = 4
	\end{rcases}
	\neq
\]
this means that $\hat y^{e} $ and $\hat y^{\xi} $ cannot be both the canonical predictor from output.
\end{exa}

\begin{rem}
\begin{align*}
	y(t+1)=e(t+1)-2e(t) \iff e(t)&=-\frac{1}{2} y(t+1)+\frac{1}{2} e(t+1)\\
	&= -\frac{1}{2} y(t+1)\underbrace{-\frac{1}{2} y(t+2)+\frac{1}{4}e(t+2)}_{\frac{1}{2} e(t+1)}\\
	&= -\frac{1}{2} y(t+1)-\frac{1}{4}y(t+2)-\frac{1}{8}y(t+3)+\frac{1}{8}e(t+3)\\
	&\quad\vdots
\end{align*}
This term is well-defined because the series converges, however we see that the noise of the non-canonical process cannot be reconstructed from the \emph{past} but from the \emph{future}! This is why it is not suitable to produce a predictor.

If instead we try to get something using values from the past we get a term which diverges:
\begin{align*}
	y(t)=e(t)-2e(t-1) \iff e(t)&=y(t)+2e(t-1)\\
	&=y(t)+\underbrace{2y(t-1)+2e(t-2)}_{2e(t-1)}\\
	&=y(t)+2y(t-1)+4y(t-2)+4e(t-3)\\
	&\quad\vdots
\end{align*}
\end{rem}

\textbf{Remark.}
In the assumptions we made at page \pageref{assumptions-prediction-theory} the really fundamental ones are that all zeros and poles are such that $|z|<1$, without this we cannot proceed; the other requests ($C(z),A(z)$ monic, coprime, null relative degree) allows for simplified formulas.

\textbf{Remark.}
Given the construction of $\hat y(t+k\mid t)$, since it's the output of an asymptotically stable filter ($F/C$) fed by $y(t)$ (which is a \gls{ssp}), also $\hat y(t+k\mid t)$ is a \gls{ssp}. The statistical properties of $\hat y(t+k\mid t)$ don't depend on $t$, thus
\[
	\hat y(t+k\mid t) = \frac{F(z)}{C(z)}y(t) \quad \text{and} \quad \hat y(t\mid t-k) = \frac{F(z)}{C(z)}y(t-k)
\]
are completely equivalent.

\textbf{Remark.}
From what we said so far, in order to compute $\hat y(t+k\mid t)$ one should know:
\[
	y(t),y(t-1),y(t-2),\ldots
\]
but in practice we only have a finite sequence back until $t=1$, the time where we started collecting information:
\[
	y(t),y(t-1),y(t-2),\ldots,y(1)
\]
so usually we use the optimal predictor, but instead of considering the steady-solution, we initialize it with:
\[
	\boxed{\hat y(k\mid 0) = \hat y(k-1\mid -1) = \hat y(k-2\mid -2) = \cdots = 0 = y(0) = y(-1) = y(-2) = \cdots}
\]
Thanks to the asymptotic stability of $F(z)/C(z)$ the effect of the conventional initialization rapidly vanishes, and is negligible provided that $t$ is large enough.

\section{Optimal prediction for non-zero mean ARMA processes}
\[
	y(t)=\frac{C(z)}{A(z)}e(t)\qquad e(t)\sim \WN(\mu,\lambda^2 )
\]
Same assumptions as in page \pageref{assumptions-prediction-theory}. How do we compute $\hat y(t+k\mid t)$?
\[
	\E[e(t)] = \mu \implies \E[y(t)]=\frac{C(1)}{A(1)}\cdot\mu=m_{y} \quad \forall t
\]
We construct the unbiased processes:
\[
	\begin{cases}
		\tilde y(t)=y(t)-m_{y}\\
		\tilde e(t)=e(t)-\mu
	\end{cases}
	\text{then (see section \ref{sec:non-zero-mean-arma})}
	\quad
	\tilde y(t)=\frac{C(z)}{A(z)}\tilde e(t) \quad \tilde e(t)\sim \WN(0,\lambda^2)
\]
so that we have:
\[
	y(t)=\tilde y(t)+m_{y} \quad\text{and}\quad y(t+k)=\tilde y(t+k)+m_{y}.
\]
$\tilde y$ is a zero mean ARMA for which we know the solution:
\[
	\hat{\tilde y} (t+k\mid t) = \frac{F(z)}{C(z)}\tilde y(t) \qquad \frac{C(z)}{A(z)}=E(z)+z^{-k}\frac{F(z) }{A(z)},
\]
However we are not interested in predicting $\tilde y$, we want to predict $y$:
\begin{align*}
	\hat y(t+k\mid t)&=\widehat{\tilde y(t+k)+m_{y}}\\
	&= \hat{\tilde y}(t+k\mid t)+m_{y}\\
	&=\frac{F(z)}{C(z)}\tilde y(t)+m_{y}\\
	&=\frac{F(z)}{C(z)}(y(t)-m_{y})+m_{y}\\
	&=\frac{F(z)}{C(z)}y(t)-\frac{F(z)}{C(z)}m_{y}+m_{y}\\
	&=\frac{F(z)}{C(z)}y(t)+\left( 1-\frac{F(1)}{C(1)} \right)  \cdot m_{y}
\end{align*}
where we used the Gain theorem (see \ref{thm:gain-theorem}). Our final solution is:
\[
	\boxed{\hat y(t+k\mid t) = \frac{F(z)}{C(z)}y(t)+\left( 1-\frac{F(1)}{C(1)} \right) m_{y}}
\]