%!TEX root = ../main.tex
\chapter{Stochastic Processes and Model Classes}
A \gls{sp} is an infinite sequence of random variables all defined on the same probabilistic space $(\Omega,\mathcal{A},\mathbb{P})$:
\[
	\ldots,v(1,s),v(2,s),v(3,s),\ldots,v(t,s),\ldots
\]
with:
\begin{itemize}
 	\item $s$: random experiment realization;
 	\item $t$: time index.
 \end{itemize}

\begin{rem}
	\gls{sp} extends the notion of random vector (\gls{sp} is a random vector with infinite entries).
\end{rem}

\begin{rem}
For a \emph{fixed} value of the random experiment $s = \overline{s}$, the \gls{sp} becomes the numeric sequence:
\[
	\ldots,v(1,\overline{s}),v(2,\overline{s}),v(3,\overline{s}),\ldots,v(t,\overline{s}),\ldots
\]
which is called \textbf{realization} of the \gls{sp}.
For different values of $s$, one gets different realizations of the \gls{sp}.
\end{rem}

We will think of available observations ${u(1),u(2),\ldots,u(N)}$ and ${y(1), y(2),\ldots, y(N)}$ as \emph{finite} length realizations.

To fully describe a \gls{sp}, we would need how the probability distribution of $v(t,s)$ varies along the time, but it is too heavy.

In our application, a \emph{wide-sense characterization} of a \gls{sp} is more than sufficient.

\begin{defn}[Wide-sense characterization]
\[
\text{Wide-sense characterization}\iff\text{\gls{sp} is described only via the mean and the covariance function.}
\]
\end{defn}

\begin{defn}[Mean value]
	The mean value $m(t)$ is the expected value of the random variable $v(t,s)$ at time $t$:
	\[
		m(t)=\E[v(t, s)]=\int_{\Omega} v(t, s) \mathbb{P}(ds)
	\]
	$m(t)$ returns the value around which the process take value at time $t$.
\end{defn}

\begin{defn}[Covariance function]
	The covariance function $\gamma(t_{1}, t_{2})$ is the expected value of the product of unbiased random variables $(v(t, s)-m(t))$ at two time instants $(t_{1}, t_{2})$:
	\begin{align*}
		\gamma(t_{1}, t_{2}) &=\E[(v(t_{1}, s)-m(t_{1}))(v(t_{2}, s)-m(t_{2}))] \\
		&=\int_{\Omega}(v(t_{1}, s)-m(t_{1}))(v(t_{2}, s)-m(t_{2})) \mathbb{P}(ds)
	\end{align*}
	$\gamma(t_{1}, t_{2})$ quantifies the relation existing between the process realizations and the mean value at two different time instants.
\end{defn}

Particular case: $t_{1}=t_{2}=t$.
\begin{defn}[Variance]
	The variance quantifies the process dispersion around its mean value at each time instant:
	\[
		\gamma(t, t)=\E[(v(t, s)-m(t))^{2}]=\int_{\Omega}(v(t, s)-m(t))^{2} \mathbb{P}(ds)
	\]
\end{defn}

\section{Stationary Stochastic Processes}
\begin{defn}
	A stochastic process is called \textbf{\gls{ssp}} (wide-sense) if:
	\begin{itemize}
		\item $m(t)=m, \forall t$;
		\item $\gamma(t_{1}, t_{2})$ only depends on $\tau=t_{1}-t_{2}$,\\
		i.e. $\gamma(t_{1}, t_{2})=\gamma(t_{3}, t_{4})$ if $t_{1}-t_{2}=t_{3}-t_{4}=\tau, \forall t_{1}, t_{2}, t_{3}, t_{4}$.
	\end{itemize}
\end{defn}
The idea is that the probabilistic properties of a \gls{ssp} are time-translation invariant.

\glspl{ssp} admit a \emph{simplified} representation of the covariance function:
\[
	\boxed{\gamma(\tau)=\gamma(t, t-\tau)=\E[(v(t)-m)(v(t-\tau)-m)]}
\]
where
\[
	\boxed{\gamma(0)=\E[(v(t)-m)^{2}]=\lambda^2} \quad \text{is the variance of the process}
\]
Why \glspl{ssp}?
\begin{itemize}
	\item \emph{Stationary} means \emph{time-invariant} data generating system (situation often encountered in practice).
	\item \glspl{ssp} are easier to study.
	\item Non-stationary processes can be recast in the framework of \gls{ssp} by first eliminating the non-stationary part from data (data pre-processing).
\end{itemize}

\textbf{Properties of the covariance function for a \gls{ssp}.}
\begin{enumerate}
	\item $\gamma(0)=\E[(v(t)-m)^{2}] \geq 0$ (non negative at initial time).
	\item $|\gamma(\tau)| \leq \gamma(0)$ (bounded).
	\item $\gamma(\tau)=\gamma(-\tau)$ (symmetric).
\end{enumerate}

\fg{0.7}{covariance-function}
\newpage
\textbf{Observations.}
\begin{itemize}
    \item Given a \gls{ssp} $x(t)$, we will write $m_{x}$ e $\gamma_{x}(\tau)$ for its mean and covariance function.
    \item Two \glspl{ssp} $y_{1}(t)$ and $y_{2}(t)$ are wide-sense equivalent if $m_{y_{1}}=m_{y_{2}}$ e $\gamma_{y_{1}}(\tau)=\gamma_{y_{2}}(\tau), \forall \tau$.
    \item The \emph{covariance function}
$$
	\E[(v(t)-m) \cdot(v(t-\tau)-m)]
$$
is very \emph{different} from the $2^{\text{nd}}$ order moment function or \emph{correlation function} $\E[v(t) \cdot v(t-\tau)]$.
\end{itemize}

\begin{exa}
$\boxed{v(t,s)=\alpha (s)}$, where $\alpha (s)\sim \mathcal{N}(1,3)$.

\begin{itemize}
	\item $m_{v}(t)=\E[v(t, s)]=\E[\alpha(s)]=1=m_{v}$\\
	doesn't depend on $t$;
	\item $\begin{aligned}[t]
		\gamma_{v}(t, t-\tau)&=\E[(v(t, s)-m_{v}(t))(v(t-\tau, s)-m_{v}(t-\tau))]\\
		&=\E[(\alpha(s)-1)(\alpha(s)-1)]=3=\gamma_{v}(\tau)
	\end{aligned}$\\
	doesn't depend on $t$.\\
	Then the process is stationary.
\end{itemize}
\end{exa}

\begin{exa}
$\boxed{v(t, s)=t \cdot \alpha(s)-t}$, where $\alpha(s) \sim \mathcal{N}(1,3)$.
\begin{itemize}
	\item $m_{v}(t)=\E[v(t, s)]=\E[t \cdot \alpha(s)-t]=t \cdot \E[\alpha(s)]-t=t-t=0$\\
	doesn't depend on $t$;
	\item $\begin{aligned}[t]
		\gamma_{v}(t, t-\tau)&=\E[(v(t, s)-m_{v}(t))(v(t-\tau, s)-m_{v}(t-\tau))]\\
	&=\E[(t \cdot \alpha(s)-t)((t-\tau) \cdot \alpha(s)-(t-\tau))]\\
	&=\E[t(t-\tau)(\alpha(s)-1)^{2}]\\
	&=t(t-\tau) \cdot \E[(\alpha(s)-1)^{2}]=t \cdot(t-\tau) \cdot 3
	\end{aligned}$\\
	\emph{does} depend on $t$.\\
	Then the process is not stationary.
\end{itemize}
\end{exa}

\begin{rem}
If $\gamma(t, \tau)>0$ then there is a tendency of preserving the sign going from $t$ to $\tau $. The opposite otherwise.
\end{rem}
\section{White Noise}

\begin{defn}
	An \gls{ssp} $e(t)$ is called \textbf{\gls{wn}} with mean $\mu$ and variance $\lambda^{2}$, we shall write
	\[
		\boxed{e(t) \sim \WN(\mu, \lambda^{2})}
	\]
	if the following conditions hold:
	\begin{itemize}
		\item $\E[e(t)]=\mu \quad \forall t$
		\item $\gamma_{e}(0)=\E[(e(t)-\mu)^{2}]=\lambda^{2} \quad \forall t$
		\item $\gamma_{e}(\tau)=\E[(e(t)-\mu) \cdot(e(t-\tau)-\mu)]=0 \quad \forall t, \forall \tau \neq 0$
	\end{itemize}	
\end{defn}
The last property is the fundamental one. It says that there is complete incorrelation between random variables at different time instants. The realizations of $e(t)$ are erratic and unpredictable (\textbf{whiteness property}).

\begin{figure}[htpb]
	\centering
	\begin{tikzpicture}
		\draw [-stealth] (-4,0) -- (4,0) node [at end,below] {$\tau$};
		\foreach \x in {-3,-2,-1,1,2,3}{
		    \node [circle,inner sep=1.5pt,fill=black,label=below:{$\x$}] at (\x,0) {};
		}
		\node [below] at (0,0) {$0$};
		\draw [-stealth] (0,0) -- (0,3) node [at end,right] {$\gamma_e(\tau)$};
		\node [circle,inner sep=1.5pt,fill=black,label=right:{$\lambda^2$}] at (0,2) {};
	\end{tikzpicture}
\end{figure}
\FloatBarrier

\begin{rem}
The probability distribution of each single random variables $e(t,s)$ does not matter and is not made explicit in general (wide-sense description of \gls{ssp}).
It could be Gaussian, uniform, etc. (WGN = White Gaussian Noise, WUN = White Uniform Noise, etc.).
\end{rem}

\begin{rem}
Is a constant realization admissible? Yes, it is, but such realization is \emph{highly unlikely}.
\end{rem}
\gls{wn} is a sort of \emph{building block} to construct a number of different \glspl{ssp}.

To ease the notation, in the following we will consider \emph{zero mean} \glspl{wn}. The extension to the general case presents no conceptual difficulties.

\subsection{MA processes}

\begin{defn}
	Let $e(t) \sim \WN(0, \lambda^{2})$. A \textbf{\gls{ma}} process of order $n$ is obtained as:
	\[
		\boxed{y(t)=c_{0} e(t)+c_{1} e(t-1)+c_{2} e(t-2)+\cdots+c_{n} e(t-n)}
	\]
\end{defn}
In other words, the output $y(t)$ of a MA process is given by a linear combination of the last $n+1$ past values of the input \gls{wn} $e(t)$.
While $t$ is let vary, the linear combination is made on a sliding window (moving average).

\textbf{Mean.}
\[
	m_{y}(t)=\E[y(t)] = \E[c_{0} e(t)+c_{1} e(t-1)+c_{2} e(t-2)+\cdots+c_{n} e(t-n)] = 0+\cdots+0=m_{y}=0,
\]
hence $m_{y}(t)$ doesn't depend on $t$.

