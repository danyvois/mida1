%!TEX root = ../main.tex

In order to solve the optimal linear prediction problem, let us start from a simpler problem, consider an ARMA process: $y(t)=W(z) \cdot e(t)$, where $e(t) \sim \WN(0, \lambda^{2})$.

Consider its MA($\infty)$ representation:
\begin{align*}
	y(t)&=w_{0} e(t)+w_{1} e(t-1)+w_{2} e(t-2)+\cdots=\sum_{i=0}^{\infty} w_{i} e(t-i)\\
	y(t-1)&=\sum_{i=0}^{\infty} w_{i} e(t-1-i)\\
	y(t-2)&=\sum_{i=0}^{\infty} w_{i} e(t-2-i)\\
	&\vdots
\end{align*}
where
\[
	W(z)=\frac{C(z)}{A(z)}=w_{0}+w_{1} z^{-1}+w_{2} z^{-2}+\cdots
\]
We can substitute our predictor:
\begin{align*}
	\hat{y}(t+k \mid t) &=\alpha_{0} y(t)+\alpha_{1} y(t-1)+\cdots \\
	&=\alpha_{0} \sum_{i=0}^{\infty} w_{i} e(t-i)+\alpha_{1} \sum_{i=0}^{\infty} w_{i} e(t-1-i)+\cdots \\
	&=\beta_{0} e(t)+\beta_{1} e(t-1)+\beta_{2} e(t-2)+\cdots\\
	&=\sum_{i=0}^{\infty} \beta_{i} e(t-i)
\end{align*}
Any linear predictor based on past output observations, can be rewritten as a (linear) predictor based on noise measurements up to time $t$, i.e.
\[
	\{\text{linear predictors from output}\} \subseteq \{\text{linear predictors from noise}\}
\]
We will solve the Optimal Predictor problem in the \emph{linear predictors from noise} set since it admits an easy solution, and then we will prove that the inclusion is actually an \emph{equality}.

The problem can be now formulated as follows:
\[
\boxed{
	\begin{gathered}
		\text{find } \beta_{0}, \beta_{1}, \beta_{2}, \ldots \text{ s.t. } \E\left[(y(t+k)-\hat{y}(t+k \mid t))^{2}\right] \text{ is minimum,}\\
		\text{where } \hat{y}(t+k \mid t) = \sum_{i=0}^{\infty} \beta_{i} e(t-i).
	\end{gathered}
	}
\]

$y(t+k)$ admits a MA($\infty$) representation
\begin{align*}
	y(t+k)&= \sum_{i=0}^{\infty} w_{i} e(t+k-i) \\
	&= w_{0} e(t+k)+w_{1} e(t+k-1)+\cdots+w_{k-1} e(t+1)\\
	&\quad + w_{k} e(t) + w_{k+1} e(t-1) + w_{k+2} e(t-2) + \cdots\\
	&=\sum_{j=0}^{k-1} w_{j} e(t+k-j)+\sum_{j=k}^{\infty} w_{j} e(t+k-j) \\
	&=\sum_{j=0}^{k-1} w_{j} e(t+k-j)+\sum_{i=0}^{\infty} w_{k+i} e(t-i)
\end{align*}
Then:
\begin{align*}
	&\E\left[(y(t+k)-\hat{y}(t+k \mid t))^{2}\right]= \\
	&=\E\left[\left(\sum_{j=0}^{k-1} w_{j} e(t+k-j)+\sum_{i=0}^{\infty} w_{k+i} e(t-i)-\sum_{i=0}^{\infty} \beta_{i} e(t-i)\right)^{2}\right]\\
	&=\E\left[\left(\sum_{j=0}^{k-1} w_{j} e(t+k-j)+\sum_{i=0}^{\infty} (w_{k+i}-\beta_{i}) e(t-i)\right)^{2}\right]\\
	&=\E\left[\left(\sum_{j=0}^{k-1} w_{j} e(t+k-j)\right)^{2}+\left(\sum_{i=0}^{\infty} (w_{k+i}-\beta_{i}) e(t-i)\right)^{2}+\right.\\
	&\quad\left.+2\cdot\left(\sum_{j=0}^{k-1} w_{j} e(t+k-j)\right)\left(\sum_{i=0}^{\infty} (w_{k+i}-\beta_{i}) e(t-i)\right)\right]\\
	&= \E\left[\left(\sum_{j=0}^{k-1} w_{j} e(t+k-j)\right)^{2}\right]+\E\left[\left(\sum_{i=0}^{\infty} (w_{k+i}-\beta_{i}) e(t-i)\right)^{2}\right]+\\
	&\quad +2 \cdot \E\left[\left(\sum_{j=0}^{k-1} w_{j} e(t+k-j)\right)\left(\sum_{i=0}^{\infty} (w_{k+i}-\beta_{i}) e(t-i)\right)\right]
\end{align*}
However,
\begin{align*}
	\E\Bigg[\underbrace{\left(\sum_{j=0}^{k-1} w_{j} e(t+k-j)\right)}_{\substack{\text{depends on }\\e(t+k),e(t+k-1),\ldots,e(t+1)}}\underbrace{\left( \sum_{i=0}^{\infty} (w_{k+i}-\beta_{i}) e(t-i)\right)}_{\substack{\text{depends on }\\e(t),e(t-1),\ldots}}\Bigg]=0
\end{align*}
All the resulting products are between uncorrelated terms
$\E[e(t+k-j) e(t-i)]=0$ (recall that we assumed $e(t) \sim \WN(0, \lambda^{2})$, i.e.
noise was zero mean).

Hence:
\begin{align*}
	\E\left[(y(t+k)-\hat{y}(t+k \mid t))^{2}\right]=\E\left[\left(\sum_{j=0}^{k-1} w_{j} e(t+k-j)\right)^{2}\right]+\E\left[\left(\sum_{i=0}^{\infty} (w_{k+i}-\beta_{i}) e(t-i)\right)^{2}\right]
\end{align*}
When we minimize with respect to $\beta_{0}, \beta_{1}, \beta_{2}, \ldots$, the first term cannot be modified, while at best the second term can be made equal to zero by choosing:
\[
	\boxed{\beta_{i} = w_{k+i}}
\]
So the optimal linear predictor based on noise measurements is:
\begin{align*}
	\boxed{\hat{y}(t+k \mid t) = \sum_{i=0}^{\infty}w_{k+i} e(t-i)}
\end{align*}
Notice that $y(t+k)$ can be written as the sum of a function of future, unpredictable from the info at $t$, and a predictable part. 
\begin{align*}
	y(t+k)= \underbrace{\sum_{j=0}^{k-1} w_{j} e(t+k-j)}_{\text{unpredictable at time $t$}} + \underbrace{\sum_{i=0}^{\infty} w_{k+i} e(t-i)}_{\substack{\text{predictable at time $t$}\\\hat{y}(t+k \mid t)}}
\end{align*}

\textbf{How to compute $w_{k+i}$?}

We perform the $k$-steps division between the numerator and the denominator:

\begin{figure}[htpb]
	\centering
	\begin{tikzpicture}

	\draw[-] (0,0) -- (0,4);
	\draw[-] (0,3) -- (2,3);
	\draw[-] (0,1) -- (-2,1);

	\node at (1,3.5) {$A(z)$};
	\node at (1,2.5) {$E(z)$};
	\node at (-1,3.5) {$C(z)$};
	\node at (-1,0.5) {$z^{-k}F(z)$};

	\node at (-1,2.5) {$\vdots$};
	\node at (5,2) {$\boxed{\frac{C(z)}{A(z)}=E(z)+z^{-k} \frac{F(z)}{A(z)}}$};
		
	\end{tikzpicture}
\end{figure}
\FloatBarrier
where:
\begin{align*}
	E(z)&=w_{0}+w_{1} z^{-1}+\cdots+w_{k-1} z^{-k+1} \\
	z^{-k} \frac{F(z)}{A(z)}&=w_{k} z^{-k}+w_{k+1} z^{-k-1}+w_{k+2} z^{-k-2}+\cdots
\end{align*}
Hence,
\begin{align*}
	y(t+k) &=\frac{C(z)}{A(z)} e(t+k)=\left[E(z)+z^{-k} \frac{F(z)}{A(z)}\right] e(t+k)\\
	&=\underbrace{E(z)e(t+k)}_{\text{unpredictable at }t}+\underbrace{\frac{F(z)}{A(z)}e(t)}_{\text{predictable at }t}
\end{align*}
The optimal predictor from noise is given by the predictable part of $y(t + k)$:
\[
	\boxed{\hat{y}(t+k \mid t) = \frac{F(z)}{A(z)}e(t)}
\]
$\hat{y}(t+k \mid t)$ is the steady-state output of a suitable linear filter fed by $e(t)$.

The prediction error is given by the unpredictable part:
\[
\boxed{\varepsilon(t+k\mid t) = E(z)e(t+k)}
\]

Infact,
\begin{align*}
    \varepsilon(t+k\mid t)&=y(t+k)-\hat{y}(t+k\mid t)\\
    &=E(z)e(t+k)+\cancel{\frac{F(z)}{A(z)}e(t)}-\cancel{\frac{F(z)}{A(z)}e(t)}
\end{align*}

\textbf{Unfortunately this result is completely useless in practice.}

In order to actually compute the predicted value for the output variable based on the above expression for $\hat{y}(t+k \mid t)$, past values of the noise process $e(t),e(t_1),e(t_2),\ldots$ should be accessible, but the output $y(t)$ is the only information in reality which we can access.

In order to use it in practice, we need to express the predicted value as a function of past values of the output variable $y(t), y(t_1), y(t_2),\ldots$.

\textbf{Is it possible to reconstruct the noise $e(t)$ from the output $y(t)$?}

If the transfer function $\frac{C(z)}{A(z)}$ is \emph{canonical} (and thus \emph{asymptotically stable}) and \emph{minimum phase} the answer is \emph{yes}.

Since
\begin{equation}\label{eq:reconstruction-noise-from-data}
	e(t)=W(z)^{-1} y(t)=\frac{A(z)}{C(z)} y(t)=\breve{w}_{0} y(t)+\breve{w}_{1} y(t-1)+\breve{w}_{2} y(t-2)+\cdots = \sum_{j=0}^{\infty} \breve{w}_{j}y(t-j)
\end{equation}
then we can substitute this expression in the predictor
\[
	\boxed{\hat{y}(t+k \mid t)=\sum_{i=0}^{\infty} w_{k+i}\left(\sum_{j=0}^{\infty} \breve{w}_{j}y(t-j-i)\right)}
\]
which is now a predictor form output and it is also optimal.

%Suppose it is not, then $\exists \hat{y}(t+k \mid t)=\sum_{i=0}^{\infty} \tilde{\alpha_i}y(t-i)$ which is better than our predictor from noise. But since the first can be written itself as a predictor from noise this is a contradiction.
%\[
%	\E\left[(y(t+k)-\sum_{i=0}^{\infty} \tilde{\alpha_i}y(t-i))^2\right]<\E\left[(y(t+k)-\hat{y}(t+k \mid t))^{2}\right]
%\]