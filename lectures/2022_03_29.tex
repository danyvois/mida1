%!TEX root = ../main.tex

\subsection{Maximum Likelihood (ML) method (ARMA/ARMAX processes)}

We consider a generic ARMAX:
\[
	\Mc(\theta ): \quad y(t)=\frac{B(z,\theta )}{A(z,\theta)} u(t-d)+\frac{C(z,\theta )}{A(z,\theta)}e(t)\quad e(t)\sim \WN(0,\lambda^2)
\]
where
\begin{align*}
	A(z)&=1-a_{1} z^{-1}-a_{2} z^{-2}-\cdots-a_{m} z^{-m} \\
	B(z)&=b_{0}+b_{1} z^{-1}+b_{2} z^{-2}+\cdots+b_{p} z^{-p} \\
	C(z)&=1+c_{1} z^{-1}+c_{2} z^{-2}+\cdots+c_{n} z^{-n}
\end{align*}
We assume that $C(z,\theta)\neq 1$. Since $A$ and $C$ are monic the first term of the long division always gives $1$ and the remainder is always $C-A$. The predictor is then:
\[
	\hat{\Mc}(\theta): \quad \hat{y}(t \mid t-1, \theta)=\frac{C(z,\theta)-A(z,\theta)}{C(z,\theta)} y(t)+\frac{B(z,\theta)}{C(z,\theta)} u(t-d)
\]
And the prediction error is:
\begin{align*}
	\varepsilon(t, \theta)=y(t)-\hat{y}(t \mid t-1, \theta)&=\left[1-\frac{C(z,\theta)-A(z,\theta)}{C(z,\theta)}\right] y(t)-\frac{B(z,\theta)}{C(z,\theta)} u(t-d)\\
	&=\frac{A(z,\theta)}{C(z,\theta)} y(t)-\frac{B(z,\theta)}{C(z,\theta)} u(t-d)
\end{align*}
Since we have $C(z,\theta)$ in the denominator, the error \emph{could be no more linear} in $\theta$ and thus the cost function could be \emph{non-convex}\footnote{i.e. non-quadratic.} and may present \emph{local} minima:
\[
	J_{N}(\theta)=\frac{1}{N} \sum_{t=1}^{N} \varepsilon(t\mid t-1, \theta)^{2}
\]
To tackle the non-linearity we can use some kinds of numerical optimization using descent methods:
\begin{itemize}
	\item the algorithm is initialized with an initial estimate (typically randomly chosen) of the optimal parameter vector: $\theta^{0}$;
	\item update rule: $\theta^{i+1} = f (\theta^{i})$;
	\item the sequence of estimates should converge to $\hat\theta_{N}$.
\end{itemize}
If there are local minima, we can just use an \emph{empirical} approach by running the algorithm with many different starting values getting a range of candidates of minimum values, hoping to explore the entire domain and not miss the global minimum. This of course comes with the cost of computational complexity.

Among the most used update rules we remember:
\begin{itemize}
    \item Newton's rule: $\theta^{i+1} = \theta^{i}-[\mathrm{Hessian}]^{-1}\cdot\nabla J_N(\theta)$
    \item gradient descent: $\theta^{i+1} = \theta^{i}-\eta\cdot\nabla J_N(\theta)$, $\eta$ scalar.
    \item Quasi Newton's rule: $\theta^{i+1} = \theta^{i}-[\mathrm{appr\;Hessian}]^{-1}\cdot\nabla J_N(\theta)$, computationally lighter than Newton's rule and more accurate than gradient decsent.
\end{itemize}


\section{Asymptotic Analysis of PEM Identification}

Is $\Mc(\hat\theta_{N})$ a good model for the process $y(t)$?
We can give an asymptotic answer as $N\to \infty$

\textbf{Assumption on the data generating system:}

$y(t),u(t)$ are \gls{ssp} generated by a \textbf{linear system:}\footnote{Not necessarily of the same type as in $\Mc$.}
\[
	S:
	\begin{cases}
		y(t) = G(z)u(t) + H(z)e(t)\\
		u(t) = F(z)r(t) + S(z)e(t)
	\end{cases}
	\qquad
	\begin{array}{l}
		e(t)\sim \WN(0,\lambda^2)\\
		r(t)\sim \WN(0,\sigma^2)
	\end{array}
\]
where $G(z),H(z),F(z),S(z)$ are \textbf{asymptotically stable, rational} transfer functions.

\begin{comment}
    The most typical cases are when:
\begin{itemize}
	\item $S(z)=0$ (\textbf{open-loop experiment})
	\fg{0.6}{loop-open}
	\item $S(z)\neq 0$ (\textbf{closed-loop experiment}), $S(z)$ accounts for $u(t)$ depending on $e(t)$ because of the feedback.
	\fg{0.6}{loop-closed}
\end{itemize}
The measured data sequence corresponds to a particular \textbf{realization} of input/output signals of $S$:
\[
	D^{N}=
	\begin{cases}
	 	u(1),u(2),\ldots,u(N)\\
	 	y(1),y(2),\ldots,y(N)
	\end{cases}
	\quad
	\text{to be thought as}
	\quad
	\begin{cases}
	 	u(1,\overline{s}),u(2,\overline{s}),\ldots,u(N,\overline{s})\\
	 	y(1,\overline{s}),y(2,\overline{s}),\ldots,y(N,\overline{s})
	\end{cases}
\]
Hence also the predictor computed from the given realization should be thought as:
\[
	\hat{y}(i\mid i-1,\theta) = f(D^{N}) = \hat{y}(i\mid i-1,\theta,\overline{s})
\]
and also:
\begin{gather*}
	\varepsilon(i,\theta) = y(i,\overline{s}) - \hat{y}(i\mid i-1,\theta,\overline{s}) = \varepsilon(i,\theta,\overline{s})
	\quad
	J_{N}(\theta) = \frac{1}{N}\sum_{i=1}^{N} \varepsilon(i,\theta,\overline{s})^2 = J_{N}(\theta,\overline{s})\\
	\hat{\theta}_{N} = \argmin_{\theta} J_{N}(\theta,\overline{s}) = \hat{\theta}_{N}(\overline{s})
\end{gather*}
\end{comment}

The case of finite $N$ is challenging: $\hat\theta_N$ chenages with the dataset! Indeed, depending on the experiment, different costs and minimizers. As $N\to \infty$, the sequence of minimizers \textbf{converges:}\footnote{In practice, a good number could be $N\geq 300$.}

\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{0.4\linewidth}
  \centering
  \includegraphics[width=\linewidth]{pem-asymptotic-fig1}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\linewidth}
  \centering
  \includegraphics[width=\linewidth]{pem-asymptotic-fig2}
\end{subfigure}
\end{figure}

Indeed we have the following result.
\begin{thm}
	Under the current assumptions, as the number of data points becomes larger and larger, we have with probability one that:
	\[
		J_{N}(\theta,s) = \frac{1}{N}\sum_{i=1}^{N} \varepsilon(t\mid t-1,\theta,s)^2 \xrightarrow{N\to\infty} \E[\varepsilon(t\mid t-1,\theta,s)^2] = \bar{J}(\theta)
	\]
	The convergence is almost sure in $s$, uniform in $\theta$ (the error goes to $0$ with the same rate for all $\theta$).\\
	Moreover if we define the \textbf{set of all minimizers:}
	\[
		\Delta = \left\{ \theta ^{\star} : \bar{J}(\theta ^{\star})\leq \bar{J}(\theta),\forall \theta  \right\}
	\]
	we have:
	\[
		\hat{\theta}_{N}(s) \xrightarrow{N\to\infty} \Delta 
	\]
\end{thm}
As a corollary we have that if $\Delta =\{\theta^{\star}\}$ (i.e. $J_{N}(\theta)$ has a unique minimum point), then:
\[
	\hat{\theta}_{N}(s) \xrightarrow{N\to\infty} \theta ^{\star} 
\]
almost surely and regardless of the experiment.

To sum up, as $N$ is large enough we can approximate $\Mc(\hat{\theta}(s))\approx\Mc(\theta ^{\star})$.

Question: is $\Mc(\theta^{\star})$ satisfactory? Let's assume that the system we are dealing with belongs to the class in which we construct the model, $S\in \Mc$. This is an ideal situation, we have enough degrees of freedom to describe the mechanism by which $y(t)$ is generated. That means that $\exists\,\theta_{0}: \Mc(\theta_{0}) = S$.